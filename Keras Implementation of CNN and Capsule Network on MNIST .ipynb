{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras  Implementation of Capsule Networks and CNN on MNIST Digits Dataset\n",
    "\n",
    "In this demo we will build both capsule and convolutional neural network model for recognizing handwritten digits.\n",
    "### Chapters\n",
    "\n",
    "####   1. Intro to CNN and Capsule Network\n",
    "####   2. Steps of building capsule network\n",
    "####    3. Importing the dependencies\n",
    "####   4. Loading and Reshaping the mnist dataset\n",
    "####   5. Building CNN and CapsNet models\n",
    "####   5.1 Training, Saving and Testing a CNN model\n",
    "####   5.2 Training, Saving and Testing a Capsnet Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intro to  CNN and Capsule Network\n",
    "\n",
    "##### What is convolution?\n",
    "In purely mathematical terms, convolution is a function derived from two given functions by integration which expresses how the shape of one is modified by the other.\n",
    "\n",
    "A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. Unlike CNN, A capsule is a group of neurons which uses vectors to represent an object or object part. Length of a vector represents presence of an object and orientation of vector represents its pose(size, position, orientation, etc). Group of these capsules forms a capsule layer and then these layers lead to form a capsule network.\n",
    "\n",
    "[References](https://arxiv.org/pdf/1710.09829.pdf)\n",
    "\n",
    "\n",
    "##  2. Steps of Building Capsule Networks\n",
    "\n",
    "#### Step One : Initial Convolutional Layer  \n",
    " This layer uses convolution to get low level features from image and pass them to the next layer of the network (a primary capsule layer).\n",
    "#### Step Two: Primary Capsule Layer\n",
    "A primary capsule layer reshapes output from  the previous layer (convolution layer) into capsules containing vectors of equal dimension. Length of each of these vector represents the probability of presence of an object.\n",
    "#### Step Three: Digit Capsule Layer\n",
    "#### Step Four:  Decorder Network\n",
    "A decoder network reconstructs the original image using an output of digit capsule layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Dense, Input, Reshape, Lambda, Layer, Flatten\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "from keras import initializers\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.core import Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Loading and Reshaping the MNIST Data\n",
    "\n",
    "MNIST data is 60,000 images stored in 28 by 28 pixel array formation. \n",
    "\n",
    "This is correct for a CNN, but we need to add one more dimension to show we're dealing with 1 RGB channel (since technically the images are in black and white, only showing values from 0-255 on a single channel), an color image would have 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download training and test data from mnist and reshape it\n",
    "\n",
    "\n",
    "(x_train, y_orig_train), (x_test, y_orig_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "y_train = np.array(to_categorical(y_orig_train.astype('float32')))\n",
    "\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(-1,28,28,1)\n",
    "y_test = np.array(to_categorical(y_orig_test.astype('float32')))\n",
    "\n",
    "x_output = x_train.reshape(-1,784)\n",
    "X_valid_output = x_test.reshape(-1,784)\n",
    "\n",
    "n_samples = 5\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = x_test[index].reshape(28, 28)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.title(\"Label:\" + str(y_orig_test[index]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building  Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.11 CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# CONVOLUTIONAL LAYER\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(28, 28, 1), activation='relu',))\n",
    "# POOLING LAYER\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "# FLATTEN IMAGES FROM 28 by 28 to 764 BEFORE FINAL LAYER\n",
    "model.add(Flatten())\n",
    "\n",
    "# 128 NEURONS IN DENSE HIDDEN LAYER (YOU CAN CHANGE THIS NUMBER OF NEURONS)\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# LAST LAYER IS THE CLASSIFIER, THUS 10 POSSIBLE CLASSES\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 23s 376us/step - loss: 0.0060 - acc: 0.9983\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 21s 349us/step - loss: 0.0062 - acc: 0.9984\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 21s 343us/step - loss: 0.0049 - acc: 0.9985\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 21s 342us/step - loss: 0.0039 - acc: 0.9990\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 22s 371us/step - loss: 0.0029 - acc: 0.9991\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 22s 374us/step - loss: 0.0028 - acc: 0.9992\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 22s 369us/step - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 23s 378us/step - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 22s 369us/step - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 23s 384us/step - loss: 0.0015 - acc: 0.9997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e160eb390>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can play around with number of epochs\n",
    "# YOUR ACCURACY MAY ALSO BE HIGHER THAN WHAT IS SHOWN HERE \n",
    "model.fit(x_train,y_train,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Displaying mode architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 25, 25, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 591,786\n",
      "Trainable params: 591,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3  Evaluation of CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 52us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10661012429856077, 0.9865]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model.save('mnist_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Capsule Network\n",
    "### Step 1: Initial Convolutional Layer\n",
    "##### Description of Convolutional Layer Architecture\n",
    "    Number of Filters = 256\n",
    "    Size of each filters = 9x9 \n",
    "    Stride =  1 \n",
    "    Activation Func:  ReLu \n",
    "    Input size of image = 28x28\n",
    "    Output size = 20x20x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_shape = Input(shape=(28,28,1))  # size of input image is 28*28\n",
    " \n",
    "# a convolution layer output shape = 20*20*256\n",
    "conv1 = Conv2D(256, (9,9), activation = 'relu', padding = 'valid')(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Primary Capsule Layer\n",
    "##### Description of Primary Capspule Layer Architecture\n",
    "The output from the previous layer is being passed to 256 filters.\n",
    "\n",
    "    Number of Filters = 256\n",
    "    Size of each Filter = 9x9\n",
    "    Sride = 2\n",
    "    Output Size = 6x6x256.\n",
    "The output is then reshaped into 8-dimensional vector. So shape will be 6x6x32 capsules each of which will be 8-dimensional. Then it will pass through a non-linear function(squash) so that length of output vector can be maintained between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution layer with stride 2 and 256 filters of size 9*9\n",
    "conv2 = Conv2D(256, (9,9), strides = 2, padding = 'valid')(conv1)\n",
    " \n",
    "# reshape into 1152 capsules of 8 dimensional vectors\n",
    "reshaped = Reshape((6*6*32,8))(conv2)\n",
    " \n",
    "def squash(inputs):\n",
    "    # take norm of input vectors\n",
    "    squared_norm = K.sum(K.square(inputs), axis = -1, keepdims = True)\n",
    " \n",
    "    # use the formula for non-linear function to return squashed output\n",
    "    return ((squared_norm/(1+squared_norm))/(K.sqrt(squared_norm+K.epsilon())))*inputs\n",
    "\n",
    "# squash the reshaped output to make length of vector b/w 0 and 1\n",
    "squashed_output = Lambda(squash)(reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Digit Capsule Layer\n",
    "##### Description of Digit Capsule Layer Architecture\n",
    "\n",
    "    Input size = 1152x8\n",
    "    Output size = 10x16\n",
    "    \n",
    "Where 10 capsules each represents an output class with 16 dimensional vector. Then each of these 10 capsules are converted into single value to predict the output class using a lambda layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-f40546104ae3>:21: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "class DigitCapsuleLayer(Layer):\n",
    "    # creating a layer class in keras\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DigitCapsuleLayer, self).__init__(**kwargs)\n",
    "        self.kernel_initializer = initializers.get('glorot_uniform')\n",
    "    \n",
    "    def build(self, input_shape): \n",
    "        # initialize weight matrix for each capsule in lower layer\n",
    "        self.W = self.add_weight(shape = [10, 6*6*32, 16, 8], initializer = self.kernel_initializer, name = 'weights')\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inputs = K.expand_dims(inputs, 1)\n",
    "        inputs = K.tile(inputs, [1, 10, 1, 1])\n",
    "        # matrix multiplication b/w previous layer output and weight matrix\n",
    "        inputs = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs)\n",
    "        b = tf.zeros(shape = [K.shape(inputs)[0], 10, 6*6*32])\n",
    "        \n",
    "# routing algorithm with updating coupling coefficient c, using scalar product b/w input capsule and output capsule\n",
    "        for i in range(3-1):\n",
    "            c = tf.nn.softmax(b, dim=1)\n",
    "            s = K.batch_dot(c, inputs, [2, 2])\n",
    "            v = squash(s)\n",
    "            b = b + K.batch_dot(v, inputs, [2,3])\n",
    "            \n",
    "        return v \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, 10, 16])\n",
    "    \n",
    "    \n",
    "    \n",
    "def output_layer(inputs):\n",
    "    return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
    " \n",
    "digit_caps = DigitCapsuleLayer()(squashed_output)\n",
    "outputs = Lambda(output_layer)(digit_caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4:  Decoder Network\n",
    "\n",
    "The purpose of  decoder network trying to reconstruct the input image. \n",
    "\n",
    "    Input Size = 10x16 (digit capsule layer output)\n",
    "    \n",
    "The decorder network will reconstruct back the original image of size 28x28. \n",
    "\n",
    "##### Number of nodes in each  dense layer of decorder network\n",
    "\n",
    "    Dense layer 1 = 512 nodes\n",
    "    Dense layer 2 = 1024\n",
    "    Dense layer 3 = 784\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(outputs):\n",
    " \n",
    "    if type(outputs) != list:  # mask at test time\n",
    "        norm_outputs = K.sqrt(K.sum(K.square(outputs), -1) + K.epsilon())\n",
    "        y  = K.one_hot(indices=K.argmax(norm_outputs, 1), num_classes = 10)\n",
    "        y = Reshape((10,1))(y)\n",
    "        return Flatten()(y*outputs)\n",
    " \n",
    "    else:    # mask at train time\n",
    "        y = Reshape((10,1))(outputs[1])\n",
    "        masked_output = y*outputs[0]\n",
    "        return Flatten()(masked_output)\n",
    "    \n",
    "inputs = Input(shape = (10,))\n",
    "masked = Lambda(mask)([digit_caps, inputs])\n",
    "masked_for_test = Lambda(mask)(digit_caps)\n",
    " \n",
    "decoded_inputs = Input(shape = (16*10,))\n",
    "dense1 = Dense(512, activation = 'relu')(decoded_inputs)\n",
    "dense2 = Dense(1024, activation = 'relu')(dense1)\n",
    "decoded_outputs = Dense(784, activation = 'sigmoid')(dense2)\n",
    "decoded_outputs = Reshape((28,28,1))(decoded_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Defining the Loss Functions and Training of model\n",
    "Defined function below is a probabilistic loss function used for classifying digits image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilitic loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    " \n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + 0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    " \n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/desire/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 777s 13ms/step - loss: 0.8138 - lambda_2_loss: 0.8137 - model_1_loss: 0.1379 - lambda_2_acc: 0.1206 - model_1_acc: 0.7906 - val_loss: 0.8095 - val_lambda_2_loss: 0.8094 - val_model_1_loss: 0.0679 - val_lambda_2_acc: 0.1009 - val_model_1_acc: 0.8030\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 782s 13ms/step - loss: 0.8095 - lambda_2_loss: 0.8094 - model_1_loss: 0.0674 - lambda_2_acc: 0.0992 - model_1_acc: 0.8036 - val_loss: 0.8095 - val_lambda_2_loss: 0.8094 - val_model_1_loss: 0.0676 - val_lambda_2_acc: 0.1009 - val_model_1_acc: 0.8034\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 758s 13ms/step - loss: 0.8095 - lambda_2_loss: 0.8094 - model_1_loss: 0.0673 - lambda_2_acc: 0.0991 - model_1_acc: 0.8036 - val_loss: 0.8095 - val_lambda_2_loss: 0.8094 - val_model_1_loss: 0.0675 - val_lambda_2_acc: 0.1009 - val_model_1_acc: 0.8022\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 770s 13ms/step - loss: 0.8095 - lambda_2_loss: 0.8094 - model_1_loss: 0.0673 - lambda_2_acc: 0.0992 - model_1_acc: 0.8035 - val_loss: 0.8095 - val_lambda_2_loss: 0.8094 - val_model_1_loss: 0.0675 - val_lambda_2_acc: 0.1009 - val_model_1_acc: 0.8037\n",
      "Epoch 5/10\n",
      "32640/60000 [===============>..............] - ETA: 5:30 - loss: 0.8095 - lambda_2_loss: 0.8094 - model_1_loss: 0.0674 - lambda_2_acc: 0.0992 - model_1_acc: 0.8031"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d455cf123d68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "decoder = Model(decoded_inputs, decoded_outputs)\n",
    "model = Model([input_shape,inputs],[outputs,decoder(masked)])\n",
    "test_model = Model(input_shape,[outputs,decoder(masked_for_test)])\n",
    " \n",
    "m = 128\n",
    "epochs = 10\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001),loss=[loss_fn,'mse'],loss_weights = [1. ,0.0005],metrics=['accuracy'])\n",
    "model.fit([x_train, y_train],[y_train,x_train],batch_size = m, epochs = epochs, validation_data = ([x_test, y_test],[y_test,x_test]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 5.2.2 Evaluation of capsule network model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predicted, image_predicted = model.predict([x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAACPCAYAAAA1FeWWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnWmsXVd5ht8v80QSO7GdxFPiOL4JiZqEqEOqFqKWtiptVKqKH1BBaAUCoapVBZ2pCm0RfzoJCaTSogpIB7WoKrQShbaQqgXEkBIs7DiD7cQhOCZOnNjO4Ey7P87x5l0v93ze6/jce/Y5932kKGvftc8e1rfW2svftKJpGhhjjDHGmO6cMu0HMMYYY4yZNbyAMsYYY4ypxAsoY4wxxphKvIAyxhhjjKnECyhjjDHGmEq8gDLGGGOMqWTFL6Ai4vKIaCLitOHxpyPitmW473si4valvs9KwrKcLyzP+cGynC8szwEzs4CKiAci4pmIOBoRByLibyLivEnfp2man26a5qMdn+fVk77/8No/OnxP/q+JiF9YivstNytMltsi4pMR8WhEPB4Rn4mIhaW417RYSfIcXv/DEXFPRLwUEW9eqvtMgxUoyxsi4s6IeHr4/xuW6l7TYKXJk+5z2/Cb+ZalvM/MLKCG3No0zXkAXgHg+wG8mytjwKy90/fQNM3/NE1z3vH/APwsgKMA/n3KjzZJVoQsAVwI4FMAFgCsA/AVAJ+c6hMtDStFngDwDQDvAPB/036QJWJFyDIizsBgLN4OYBWAjwL45PDv88SKkOdxImIVgN8BsGOp7zWTjdY0zcMAPg3guoi4IyLeFxFfAPA0gC0RcUFEfCQi9kfEwxHxxxFxKgBExKkR8ScRcTAi9gD4Gb728HpvoeO3RsTdEXEkInZGxCsi4uMANgH41+HK/jeH5/5QRHwxIp6IiG9ExC10nSsi4r+H1/kPABdXvPJtAD7RNM1TYzVYj5l3WTZN85WmaT7SNM3jTdM8D+DPASxExEUTasJeMe/yHL7jB5um+S8Az06izfrKCpDlLQBOA/AXTdMca5rmAwACwI+ddOP1kBUgz+O8H8AHABw8mfbqRNM0M/EfgAcAvHpY3ojB6vKPANwBYB+AazEYDKcD+BcAfwngXABrMfhX/9uGv307gF3Da6wG8HkADYDThvV3AHjLsPw6AA9jsGoPAFsBbNbnGR6vB/AYgNdgsDD9ieHxmmH9lwD8GYAzAbwSwBEAt9PvtwN4wyLvfc7w3FumLQPL8uRkOax7LYD905aB5TmRsfm/AN487fa3LMeTJYBfB/Bpef9/A/DOacvB8hxvbAL4AQBfG16rfaYla99pC7iyIxwF8ASABwF8CMDZw0b6QzpvHYBjAM6mv70ewOeH5c8BeDvV/WTSET4D4NdO1DGHx78F4ONyzmcw0B5tAvACgHOp7u+4IyTv/UYAewHEtGVgWZ60LDdgMLG8ftoysDwnIs95XUCtCFkC+H0A/yB/+1sA75m2HCzPseR5KgaLp5v1mZbqv9MwW7y2aZr/5D9EBAA8RH/ajMFqev+wDhisRo+fc5mc/2Byv40Adnd8ts0AXhcRt9LfTsdgpX4ZgENNaYJ7cHj9E3EbgI81wx4xR6woWUbEGgCfBfChpmn+vuNzzBIrSp5zzkqR5VEA58vfzsdAyzFPrBR5vgPA9qZpvtTx3ifNrC2gRsGLi4cwWElf3DTNC4ucux+lADYl130IwJUd7nn83I83TfNWPTEiNgNYFRHnUmfYtMg19HcbMbDTvy07b86YO1nGwKnxswA+1TTN+5JnnEfmTp4rmHmT5Q4A74yIoH+gfh+ADybPOk/Mmzx/HMCrIuI1w+PVAG6MiBuapvmV5HnHZiadyDOaptmPwcfqTyPi/Ig4JSKujIhXDU/5RwC/GhEbhh+2304u99cA3hURN8WArUOhAsABAFvo3NsB3BoRPzV0uDsrIm6JiA1N0zyIgWrxvRFxRkT8CIBbcWLeCOCLTdN0Xc3PFfMgy4g4HwOV9Beapsmeb+6ZB3kCg+itiDgLA/+O04fXm7u5NGNOZHkHgBeHz3lmRBz/yH6uoinmgjmR55sBXAPghuF/XwPwXgC/V9MWNczroH8TgDMA7ARwCMAnAFw6rPsrDD5o38AgDPmfR12kaZp/AvA+DOyuRzBwsls9rH4/gHfHIHLgXU3TPATg5wD8LoBHMVhZ/wa+28ZvAPCDAB4H8AcAPsb3iogdEfGLi7zHCXNrzDmzLsufx8CZ8peizOuV/Qtunpl1eQKDD80zAH4YwIeH5VfWNMKcMNOybJrmOQyCOt6EgY/QL2Ng7npujLaYB2Zdnk80TfPI8f8APAfgcNM0T47XHCcm5s+1xhhjjDFmaZlXDZQxxhhjzJLhBZQxxhhjTCVeQBljjDHGVOIFlDHGGGNMJV5AGWOMMcZUstyJNB3yN33ixKecmK6Z0fW0+G6W26K82Lldrznq+nqc/Y7rsmc+5ZTy3xwvvfTSyHtn8D30mqPOU6Lmhh0eaYLXMuMxKXm2sqyJsuZzs66l1+T+uxRR3V2fa9rXZDw2545F5WkNlDHGGGNMJfOylYtZZrr+C061KzXao3Ho+i/nTFOVaZky7VQNmdaJmdT9zMqhq6a1pi91PXc5tMrjsBzjyGNz5WENlDHGGGNMJV5AGWOMMcZU4gWUMcYYY0wl9oE6CcaNOJkHW3nXyDFto67+GUoWvcc+SlzOnuWFF14Yef3s3qeffvrI63f1a9LfZX4j89BXTH/IxtGo84DufVLHX3bNrr/rWnfaaeXnjMdjzftkkbtZ3Uoft6Pm70n5t/WxTa2BMsYYY4ypxAsoY4wxxphKYikSnyX0JiEYv3dm9slUxOPcC8jV6MugppzIDV566aX2pWpU4JnJa1zT39GjR9vyM888U9QdOHBg0WvoefwsZ555ZlH3spe9rC2fccYZRd0555zTlk899dSi7rzzzhtZNwl1/0pI1pfJncdm1pdqkkJ2NXEtERNPpKlkiV+7Jp39npslc+mLL77YljOzOZ+nz1VjGmf4fmp6P+uss8a6ZoY891yOzUyeo2RY47bB86T2Je4HOg8vw7h1Ik1jjDHGmEngBZQxxhhjTCVeQBljjDHGVDLXaQzYJqt0tcuy3TWz4ev1+Fy14We/Yz8r9ZvpUxhn15DkzM+iJuSZ/Zz27NlT1H3rW98aWffAAw+05UOHDrVl9YF67rnn2jL7LgHAhg0b2vLCwkJRd8UVV7TlSy+9tKjjPnDBBRcUdZlfR5/kvBxkfjNZHcuQ+wcAHDlypC1nfk4ql9WrV7dl9bNQP5q+kvmGdfUHy3wQte75559f9N5AOQb4PKCUH8vriSeeKM7jYx6nQDlfrlmzpqi78MIL2zL7MeqxylXlzqyEsckyy2TdlZq+xP1Hv7dc9+yzzxZ17NOm8tTv6CSxBsoYY4wxphIvoIwxxhhjKpl5Ex6rAFW9OG5G2VHmlZpwWlYjqimRVZFaxypqVUVyeP24ob1LgartWWVaEw791FNPteXDhw8XdXv37l20DAD33XdfW77nnnuKun379rXl73znO2356aefLs5jOXBqAqA0zR08eLCoY9ODyiRTHbMsNaSar5OlP5gltI90zR6v44PlxvLcv39/cd7jjz/elnVeYLOPyvqiiy5qy2vXri3qNm3a1JZrZL3cZGZyfu6aOZGvo+3Jpha9H49pNrXrMY9pNcOzbNVUy2PnsssuK+q2bdvWlq+55pqi7vLLL2/LF198cVF37rnntuWzzz4bXZmlTOSZqSxLKZG5XXSdm7L+yWZdNdPxsY4/NsVrGhqWp6YiOlk59ecrbIwxxhgzI3gBZYwxxhhTiRdQxhhjjDGVzJwPVGabz+rUlyKz27MfUtcQXbWlsq1Vw2LZXyJLVaB+Oly3nKGaJ2Jcf6xMJpxyAMj9XR599NG2zD4XQNknOHRZbeEsc32fzI+DfWbWrVtX1HFIvNrlMx85rtM+13ffCibzZRp1nh5nPhjsL8F9ACj7i8qTfSL03uxLob4+PB7VN4afc9r+ieP2kWxLlsz/hH+nKQgeeuihtqz+iTt27GjL7MfIfotAKVvtR/wsLFe9t/YPfp+rrrqqqONxrPMqj+PMh6xv6LMeO3ZsZB23sc6nPAa0jn/H7aY+nnw/9XPief+xxx4r6rh99ZqrVq1qy5pOhvuypqjJUlZ0wRooY4wxxphKvIAyxhhjjKlk5k14rPLj0GWgDDnXOlY/qnqX1ZtcVpMaoypvVvWef/75RR2HzW7cuLGo46zXqm7k++s1+0QWzsrtpCpglgOnBwByExub5jT0nLMRs1pZMxrzc6n5kNXMKucnn3yyLavKmUOuNeM1q7u5jwGlCa9P4fG18DvWmPCyMGe+DrfvgQMHivPUzMtccsklbVkzVHPbq9k1y6DM5mc1ry+3aSd7zlHn6bGGtjM6djjlCGf+B4Bdu3a15bvuuquoY7Pdww8/3JZ5TOn9alJe7N69uy2ryZ7nVjXHsilQ+0dGn014ao7mOUdlzWZYlQV/R3U3B+733L5qbmP0m8rjVlPG8DtkctHUJCz7Gnl2wRooY4wxxphKvIAyxhhjjKnECyhjjDHGmEpmzgdK7e9sV2ebOlBuD6A+EWxzV1sr233Z/0VDLhm1zbPdV7cK2Lp1a1u+6aabirqbb765LWt4Ldvqpx0qnYWaZ34XfK7+jkNK1Y+E7drswwKUPgu83QZQthNfX+3+7MfBvhNAvgUM/063n2EfncwHSGWZ+QBl7TdtsjQi2bn6Htl78TXZT07HMI93DVVmPzmtY98NDYtnXwqVJ9dNWy7ctjXzBP9O51mu0zGQpSrYvn37yLpHHnmkLbNPjvoLst9Ktu2RplBgHx2e74HyW6F+k+vXr2/LKueu8960+wCQp6Xg91JZs5/Tt7/97ZF16rvJYyfbDoZlqNdgGarMMrgf6LeDx+akv5vWQBljjDHGVOIFlDHGGGNMJTNhwmNVpIZVctisqogzEx6bZTRkntWPrA5U9R8fqyqSTQq6gzirU9esWVPULSwstGUNPe2DWvg42bN0NcFom7GZTlX1vNO6thnLoevu83pvNieoqZbVyllmXr1mFk7Oz6kh1nyupjHouuP5NMhMdjp2+NwsVUOW9ZpND2rCY3mqyZfNdpzmQo81vJ2fU01/fRqbo/r8ycDy4rkTKM1h2RysaT54buUxzWNdj1VePFezKREA7r333ras5nU2TenuApzGJOvTNa4L00bNdNxuavpk+aoJj1PNZOYwlqemA+FxpK4UPI41NQn3F3WL4To1vWdpFE4Wa6CMMcYYYyrxAsoYY4wxphIvoIwxxhhjKpkJHyi2NatfAvuPaN1FF13UltVey7s3qw8Gb5PCfjlqW83s79/85jfbsm4Nwu+ThVyq7TjbCXy56ernpOdlvi98zLIDuocP6/YCo+6dpSPIdi7X63OfUFny++i7Zr51/H7TlnMNKpeuvk1ZGoNsu5FsKxf249Dd2dmPZt26dUUdj331TdNjZtppRcahZgsdHh8PPvhgUbdv3762rFu5sBy0P7BctmzZ0pa3bdtWnMd1muKAfXvuv//+oo7fj+djoOw76h+rvlpMn3zdalB58pymPlDsh6Q+UHyu+qPxt5Lr9LzMD5bvrc/F32z9NvK3X+dhPXeSzN6oN8YYY4yZMl5AGWOMMcZUMhMmPEZV5ZxFVtMFcAZbDZdkNZ+GOXMIJqsfNXSZUyp8+ctfLupY/aj35uditaTeT1WfmQlhuWE1bI35gs/NTHhqjs12XR91faBU8We7unNdltFYn5lDZLPwWf1dZo5dijD0adA1a3pmptOxw3LjlBIais5jU3dnZ9NRFg6t443l0meTXZY+oyuayoPD19lkBwB79uxpy2oOY3mpKfX6669vy9dcc82iZaA0s6osuQ/omOY+oVmt2a1CTXZ8rO3Ac3ff4TGnZiw2rbJsgbLddFzxeNTvJh+vXr160TJQmtv13mwy1PRCo77LQCkX7SNLaXbt7yxgjDHGGNNTvIAyxhhjjKnECyhjjDHGmEr641TTEfVDYh8GtYGzT5T6M/Dv1A+CbbZsO1Z7ONepjwuHZ+ozb9iwoS1v2rSpqGM/gWy7iGn7xoy7lQuT+b5oKGq2pULmE8W/Y/s97yoOlL4bWsdb6qgvAfcd3WKGdyfX/sfXnKXtWjKybVcytL9k8mQ4dcju3buLOvahUx+M9evXL3oekPufdfXp6hM1KSK43VUG7EOkfjF8rHMk+3heffXVRR37QHHqAt3KhWWk8wDX8XgDSv9Yrcv8nNivSrfTytKp9BmdT/kddRskTlOhPm08ltTXaFSaAd1+jedXTT3BPlA613JqG01nwd/KmvQjJ4s1UMYYY4wxlXgBZYwxxhhTycyZ8HRXaVZNavZqVtvqjsysbtTf8TUzcwKHwvKO5EAZqqnh7WzCu+KKK4o6DtnNdrDvUxh1ZorLnlNVq2xiU/U416lanfuEquM5gzLLRE0+LD8NreV21/BZNjdoSgruc/rMrP5Ws0TWfrNk3qtJXTCqTtuGQ5vvueeetqypJ3jMsckOKE2tXbOlA7NlshlFluKA30/NLmwu1V0X2Bym7bl58+a2rOkJrrzyyrbMMtL5kuWg8z8f61zN84u6Q2RmHe5zer+MPvcPzfidpQDhuVHNe5k5lU1z2e4Nu3btastf//rXizo2H+q3kWWmJklG+wF/O7LxPg79+QobY4wxxswIXkAZY4wxxlQyEyY8VjWr6pW98VXlnmWJ5mNV+Y3KOKwb0O7cubMt33333UUdRwCqCYFNeBoBmKkYs80+p2nSq1Fd83NnkXVqimMTjZoX2JSqKmc2L3CEh256ynUqZzYFqwmPzb/ax7ivqlz5/TJ1dGYe7bPJ4ERkfZmPdWyymYBNSXoN3hR469atRR2bITIz3ay2bxZpl70Tmzp0c2YeH7rBLJtVeeNfoDTDLCwsFHUcgcyZpDPXBYVN42omZ9OR9iMej+rewXXjbvTdh76TtRtnIs/mQjb1AWW/2LFjR1HH8zJH0OmczN/K7du3F3U8z2vmep4z1STJfTd770m7QFgDZYwxxhhTiRdQxhhjjDGVeAFljDHGGFPJTPhAMbo7O/uPaLZZtoVmYatqrx4Vxqrhu1/96lfb8t69e4s6tqOrDxSHvmsKhew5u6YHWA4yX4CuIeoK+1LoDulsp9fd4Nkur1nEOYMyl/X67D+hWW45PYH6OXE6AvVz4muqvFjONdmi++BbMQmyfsDtqH4tHOasvhUM+97o+MuyjWf+lrPIuP2FfWSAcryoDyK30yWXXFLUXXXVVW1Z5cBjh6+hMuH5WPsD+5rqmH7kkUfasvrysI8O+8vpcfZtyMZtH+DnUR8wbmP9pvL40DpuY04jApRjk++nKUY4u7mmjMnGJste67I5g9cImt38ZLEGyhhjjDGmEi+gjDHGGGMqmQkddRa6zSHJmm2W1YiZKUTVmxwiyeahO++8sziPM6pqxlo2IegmmpzGIMu6m22yOm11cabK5nfQtuV2UrU6Z8S99957izo+ZlMcMDpVAVCqlTndgfYHNtvVbFTJ6mJNf8D3UzMBq5Uz01+W4mBeUFmw2UBNSRw6zf1HQ9E567W2fca42dOnPR6ZcUO1M3MYm0t1THP7aug5Z33XFCDc73k+0efn8HUdY/xcOvbZ5ULTovB3Q3cQ4NQyumFu5jrRpz4AlO2o2cD5/TX1BI8rNb8xOheybLLdIXh8ZzuE6KbS3Ld0g3B+Fn0ullmW4mAcrIEyxhhjjKnECyhjjDHGmEq8gDLGGGOMqWQmfKDY5l4TDp7Zq7Mdvjmcfs+ePW1Zd45m/wy29QPAtdde25Y5lBcodyHPtv/InnnadA0FV5mwfw+3MwDcf//9bZn9y4BSDhwGC5Q2ew2L5bosRJ19AtQuz++gPhgcKq3XZHu++o3wNTX9BjMrfjdAnX9BFpp++PDhtsxyB0o/F+6DunP7tm3b2rL6kXVtt5oUEn3dYkefhZ9T34/bk/2hgNIvVEPbMz9UHkvq6zcqlF7nYx5zPN6Asn/o1iI6TzDsQ7Nx48aijlMx6Pv0Vc6LwW2qc8zll1/elnXccroJTQuT+bfyuOVUF7pVDD+Lynrz5s1t+eUvf3lRx9/NtWvXFnXsX5eNd/2Gnqw8+/NFNsYYY4yZEbyAMsYYY4ypZCZMeKxiVDUwq49VJa0qxlHoeWya4/B5VR/zs6gJgcOoWV0KlGG/+j59MtN1JcvWm4Uka9gxpyfglAZAaZrj9ABAaW5Q0wObhzgLLe/+rseqAh6V1gLITVFsotQ6bjPNRs9m3b5nO2ZqMtDzmMvSWaj6n+XL4eZqJmeVfo2JOXvm7Hd9Hbdqnsmy3OvYGXUdvSabYdTEzWHwGhLPKQL4Gmra53n3vvvuK+rYrUIzY3O/0gzpmzZtass6d7N5SM3yWR/om3kvex52OeFvKFB+rzQFAV9T69jcx+ZTzf6djU1O8cNmeKBMa6CpZrLx19UFaBz6OeqNMcYYY3qMF1DGGGOMMZV4AWWMMcYYU8lM+EBlIfNZ3ajzgNKOrz4vbL9l+7v6CHAorPpgbN26tS2rjT3bGiSznWfbpyw34+5KzlsK6HYR7COh4bPse6S+aHxN9lfSZ2E/Jw3rZb+jbPsGDeHOtjrgNlI/APZByHZKn7acT0Q25vj9td3Yf0JlzeNP25fD4jmsmX0ngFxmTE2aiMwXs0907TOZD4/6CGpqD4bH8d69e4s6Ti2gY5PvwWNA/Qx5Oyb1geI69aVjHznuK0A+P2c+qn0fj13h+Ui/QTw3ZnOT+qIy7Mem265wG2qaiHXr1rVl3fqHj1UOLCf91vOx0xgYY4wxxkwZL6CMMcYYYyqZCRNeRmZKGrXbN1Cq9VXtzCYEDp9XNTaHVWrIJWdUzchSLahpkdWP01Yld72/hu/zcRYKrvIaleVWz1V19Kid4lWtzCY1zQ6fZd/lPpbJS0OF2XzBzwiU7aLt0Kc+cCJY1toPsh3f2VSuJhQ2y/AYVnV/Fp7MantV6Xdt+8z0N23G3XGezTo8VoDStKLzJacfueuuu4o6Hre6uwCPM35mNeFxWhl1o+Df8TMCZSbrq6++uqjj+ZozbwPlOM7cE/o2/mrM0TyuauZhNcMyPB55/Kk5mO+t7hI8L2tdNm6zeTHLRH6yMrQGyhhjjDGmEi+gjDHGGGMq8QLKGGOMMaaSmfCBYjtsFoaoZHUcOr179+6ijm3wbCNVWzmHxrI/FFCGgqrvFPuDqK8Gn6v+NnzutO3v2ZYQmc2ZbdzszwKUW5robtvcvuoXw9tAaIp/lhn7pam/BPshacoB7n8qE35XlTO/w8LCQlHHPleZv06fw+WBUvZZP9B24y0/OA2F1mk/X7Vq1aL3077EMtQ0Blk6gsx/KPN/mfZ4ZLo+i74rh5Tr9lPsM6TbLB08eLAt79u3r6jjrXjUT419oLKtOFhGOr75OdnnCSj9ntRHlVMX6Ljl+2Xjr8bnaBp07a81/nw813YdKyozbm+da/lY/Ua5f9akl1hKvzVroIwxxhhjKvECyhhjjDGmkpkw4bHqTsOhszQAjIZKc0ZbDZtllSabDFSlyOYhDUVnFaM+c5YZld9Hw+L7oBY+TtcM1Pp+HJqqO6Rff/31bVnV6hxWnaUx0JBZzlDNZjNVK7M5QduZ5cAqbKBsB1VHswmP7w2UJt4sjLjvZoJxdwngTORssgPK99JxxXLj8zTkmeeMrJ30ufhYTauzlELiOF1NG0A+Nm+88ca2zLJT1LTCJj2dgzmVBd9bU4yw6V13fGAznaYq4LGv75OZg7I0Bn0mS1WQzSPaz/l7lZnlM3j+1t/w/fTePA/r95a/jfpN5XN1HrYJzxhjjDGmR3gBZYwxxhhTiRdQxhhjjDGVzIQPFNst1V7NdlENV2abu/o5ZTu+s32Vbbnqx5L5R7Ffjtpr+Zrq65OFtM8Kme8L+x7odhHsv7Rx48aijv1ksrB0tX9z+2Zh0139WzKfGd1ZnP139H5ZeHS2PVGf0XfittE6Hi/qt8bjJds6J9sqh8eq+lFl6UD4mjWh0n2i63Y0CreLphy47rrr2rL6D27ZsqUt79y5s6jbs2dPWz506NDIe3MKE00Xw35PukUWzyH6zDwX6NjkY/U15X47qfD/adB1jtH5NOs/2RgY5UeqPo7q48Zkz8x1Omfo+GeWctxaA2WMMcYYU4kXUMYYY4wxlcQyqyEnfjN+fg6LBUoznWbI5WPNrMuZkVnVq9mr+VgzIbOaW7Nqs/qRw9mXiYnoM5sxO06WuZqPjxw5UtSxKlnNnFnahFEhrGoaYjW+qq2zZ87MSJlZMDMTdCUmq5te0olA25TNdBoWryZvZpQJT8nC1LteYwomu0ndsJVlTRh6No44fcexY8dG/k7TfPCcrHUMj78sO7Wa27pm6ldXia5y75pte5G6Xo9N7hc6/vhYzXt8rOOUv5vZGObrqxx4bmcXGaA00ep3cxnG6qI3sAbKGGOMMaYSL6CMMcYYYyrxAsoYY4wxppKZj5ln26faRbdu3dqW1c7LqQvU5s72YfZl0u0AOBxTdy/nUNzML2el0XWbEg0953PZ1g6U/g2ZX1X2d5aJ2v3Zn0Z9KZjMr0LJwqFrtt+YFbRtMr+WrmkcurrhZSkr5pHMvy5r264pDtQHkf0J1Scp27JoFOpjxeNPx2a27cq4KWEm4Z/Yd/i91HeT5zhN8cO/6/pdU38o/m5qf+F0FrMwbvv3RMYYY4wxPccLKGOMMcaYSmY+jcG4dFXTzqE6d+JpDDITVGZCyFSyWb+sqRt1rt47u+Yk0hFkGaHHHYN9D5U21Uw8jcH3VCT9dVzTX9ff1TzLKGrGLVOTUXzcYSXvM5djM5u/x5XFqGv07HvrNAbGGGOMMZPACyhjjDHGmEq8gDLGGGOMqWTF+kCtYJZ8K5fMv6Crr1G2nYqGvnIYdebrsBQ7qXfduXxSW7msBD+LFcxUt1maBFk/X4prZOOo63ifRKqMRa7psTlf2AfKGGOMMWYSeAFljDHGGFPJcpvwjDHGGGNmHmugjDHGGGMq8QLKGGPjZemiAAAAvElEQVSMMaYSL6CMMcYYYyrxAsoYY4wxphIvoIwxxhhjKvECyhhjjDGmEi+gjDHGGGMq8QLKGGOMMaYSL6CMMcYYYyrxAsoYY4wxphIvoIwxxhhjKvECyhhjjDGmEi+gjDHGGGMq8QLKGGOMMaYSL6CMMcYYYyrxAsoYY4wxphIvoIwxxhhjKvECyhhjjDGmEi+gjDHGGGMq8QLKGGOMMaYSL6CMMcYYYyrxAsoYY4wxphIvoIwxxhhjKvl/aHzyM2zCwoUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 5\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = x_test[index].reshape(28, 28)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.title(\"Label:\" + str(y_orig_test[index]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = image_predicted[index].reshape(28, 28)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.title(\"Predicted:\" + str(np.argmax(label_predicted[index])))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model.save('mnist_capsnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "\n",
    "from keras.models import load_model\n",
    "# Assuming your model includes instance of an \"AttentionLayer\" class\n",
    "model = load_model('mnist_capsnet.h5', custom_objects={'DigitCapsuleLayer': DigitCapsuleLayer,'loss_fn':loss_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
